+++
title = "Concurrency & Parallelism"
[extra]
index = 13
[[extra.readings]]
name = "Threads cannot be implemented as a library"
url = "https://dl.acm.org/citation.cfm?id=1065042"
details = "Hans-J. Boehm. PLDI 2005."
[[extra.videos]]
id = "1_8cpusna2"
+++
## Gist

* The end of Moore's law rears its cliché head. The need for parallelism to sustain advances in computing.
* Kinds of parallel programming models.
    * Message passing.
    * Shared memory. (The predominant model circa 2020.)
    * Data parallelism.
    * Task parallelism.
    * Actors.
* The problem of semantics in shared-memory multithreading, as demonstrated by an imaginary extension to Bril.

Imagine this setup code:

    zero: int = const 0;
    one: int = const 1;
    a: ptr<int> = alloc one;
    b: ptr<int> = alloc one;
    store a zero;
    store b zero;

that stores 0 in two pointers, `a` and `b`. Then imagine these two threads running concurrently:

    # thread 1
    store a one;
    bv: int = load b;

    # thread 2
    store b one;
    av: int = load a;

What are the possible values of `av` and `bv` at the end?
If you said the set {(0, 1), (1, 0), (1, 1)}, then you're implicitly imagining that parallel execution looks like an *interleaving* of the instructions from parallel threads.
That would be nice!
It's a *memory model* called *sequential consistency*.
(For more on memory models, I strongly recommend [a primer by Sorin, Hill, and Wood][primer].)

* Sequential consistency and its discontents.
    * The happens-before relation, due to [Lamport][].
    * Demonstrating a [C program][scviol] that violates SC.
* Memory consistency models, more generally.
    * Defining data races.
    * The C++11 memory model and DRF⇒SC.
    * The Java memory model's long tale of woe.
* [Threads cannot be implemented as a library.][boehm]
    * Attempt 1: Just compile programs normally, I guess? The lesson is that there need to be special "privileged" operations for synchronization.
    * Attempt 2: Synchronization operations are "black-box" calls that can't be analyzed interprocedurally. Some very tricky examples [due to Boehm][boehm] where things can go wrong: when optimizations that are *perfectly fine* in a single-threaded context are *completely incorrect* in a multi-threaded context, even when there are no threading calls anywhere in sight.

[lamport]: https://lamport.azurewebsites.net/pubs/time-clocks.pdf
[boehm]: https://dl.acm.org/citation.cfm?id=1065042
[primer]: https://course.ece.cmu.edu/~ece847c/S15/lib/exe/fetch.php?media=part2_2_sorin12.pdf
[scviol]: https://github.com/cs4110/2018fa/blob/master/code/code32/scviol.c

## Tasks

There are no tasks for this lesson other than pondering the deep weirdness of programming with shared-memory multithreading.
Good luck on your course project!
